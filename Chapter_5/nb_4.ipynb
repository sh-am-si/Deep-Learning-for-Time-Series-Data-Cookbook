{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "746ac8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volody/code/study-py/ts-pytorch/.venv/lib/python3.13/site-packages/gluonts/json.py:102: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightning.pytorch as ptl\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from gluonts.dataset.repository.datasets import get_dataset, dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4126ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['constant',\n",
      " 'exchange_rate',\n",
      " 'solar-energy',\n",
      " 'electricity',\n",
      " 'traffic',\n",
      " 'exchange_rate_nips',\n",
      " 'electricity_nips',\n",
      " 'traffic_nips',\n",
      " 'solar_nips',\n",
      " 'wiki2000_nips',\n",
      " 'wiki-rolling_nips',\n",
      " 'taxi_30min',\n",
      " 'kaggle_web_traffic_with_missing',\n",
      " 'kaggle_web_traffic_without_missing',\n",
      " 'kaggle_web_traffic_weekly',\n",
      " 'm1_yearly',\n",
      " 'm1_quarterly',\n",
      " 'm1_monthly',\n",
      " 'nn5_daily_with_missing',\n",
      " 'nn5_daily_without_missing',\n",
      " 'nn5_weekly',\n",
      " 'tourism_monthly',\n",
      " 'tourism_quarterly',\n",
      " 'tourism_yearly',\n",
      " 'cif_2016',\n",
      " 'london_smart_meters_without_missing',\n",
      " 'wind_farms_without_missing',\n",
      " 'car_parts_without_missing',\n",
      " 'dominick',\n",
      " 'fred_md',\n",
      " 'pedestrian_counts',\n",
      " 'hospital',\n",
      " 'covid_deaths',\n",
      " 'kdd_cup_2018_without_missing',\n",
      " 'weather',\n",
      " 'm3_monthly',\n",
      " 'm3_quarterly',\n",
      " 'm3_yearly',\n",
      " 'm3_other',\n",
      " 'm4_hourly',\n",
      " 'm4_daily',\n",
      " 'm4_weekly',\n",
      " 'm4_monthly',\n",
      " 'm4_quarterly',\n",
      " 'm4_yearly',\n",
      " 'm5',\n",
      " 'uber_tlc_daily',\n",
      " 'uber_tlc_hourly',\n",
      " 'airpassengers',\n",
      " 'australian_electricity_demand',\n",
      " 'electricity_hourly',\n",
      " 'electricity_weekly',\n",
      " 'rideshare_without_missing',\n",
      " 'saugeenday',\n",
      " 'solar_10_minutes',\n",
      " 'solar_weekly',\n",
      " 'sunspot_without_missing',\n",
      " 'temperature_rain_without_missing',\n",
      " 'vehicle_trips_without_missing',\n",
      " 'ercot',\n",
      " 'ett_small_15min',\n",
      " 'ett_small_1h']\n",
      "111\n",
      "735\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset_names)\n",
    "dataset = get_dataset(\"nn5_daily_without_missing\", regenerate=False)\n",
    "\n",
    "print(len(list(dataset.train)))\n",
    "print(len(list(dataset.train)[0][\"target\"]))\n",
    "\n",
    "N_LAGS = 7\n",
    "HORIZON = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "544e379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogTransformation:\n",
    "    @staticmethod\n",
    "    def transform(x):\n",
    "        xt = np.sign(x) * np.log(np.abs(x) + 1)\n",
    "\n",
    "        return xt\n",
    "\n",
    "    @staticmethod\n",
    "    def inverse_transform(xt):\n",
    "        x = np.sign(xt) * (np.exp(np.abs(xt)) - 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db86c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalScaler:\n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        df = df.copy()\n",
    "        df_g = df.groupby(\"group_id\")\n",
    "        for g, df_ in df_g:\n",
    "            scl = StandardScaler()\n",
    "            scl.fit(df_[[\"value\"]])\n",
    "\n",
    "            self.scalers[g] = scl\n",
    "\n",
    "    def transform(self, df: pd.DataFrame):\n",
    "        df = df.copy()\n",
    "        df[\"value\"] = LogTransformation.transform(df[\"value\"])\n",
    "\n",
    "        df_g = df.groupby(\"group_id\")\n",
    "        transf_df_l = []\n",
    "        for g, df_ in df_g:\n",
    "            df_[[\"value\"]] = self.scalers[g].transform(df_[[\"value\"]])\n",
    "\n",
    "            transf_df_l.append(df_)\n",
    "\n",
    "        transf_df = pd.concat(transf_df_l)\n",
    "        transf_df = transf_df.sort_index()\n",
    "\n",
    "        return transf_df\n",
    "\n",
    "    def inverse_transform(self, df: pd.DataFrame, col_name=None):\n",
    "        df = df.copy()\n",
    "        if col_name is None:\n",
    "            col_name = \"value\"\n",
    "\n",
    "        df_g = df.groupby(\"group_id\")\n",
    "        itransf_df_l = []\n",
    "        for g, df_ in df_g:\n",
    "            df_[[col_name]] = self.scalers[g].inverse_transform(df_[[col_name]])\n",
    "\n",
    "            itransf_df_l.append(df_)\n",
    "\n",
    "        itransf_df = pd.concat(itransf_df_l)\n",
    "        itransf_df = itransf_df.sort_index()\n",
    "        itransf_df[col_name] = LogTransformation.inverse_transform(itransf_df[col_name])\n",
    "\n",
    "        return itransf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13362dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalDataModule(ptl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, data, n_lags: int, horizon: int, test_size: float, batch_size: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.test_size = test_size\n",
    "        self.n_lags = n_lags\n",
    "        self.horizon = horizon\n",
    "\n",
    "        self.training = None\n",
    "        self.validation = None\n",
    "        self.test = None\n",
    "        self.predict_set = None\n",
    "\n",
    "        self.target_scaler = LocalScaler()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        data_list = list(self.data.train)\n",
    "\n",
    "        data_list = [\n",
    "            pd.Series(\n",
    "                ts[\"target\"],\n",
    "                index=pd.date_range(\n",
    "                    start=ts[\"start\"].to_timestamp(),\n",
    "                    freq=ts[\"start\"].freq,\n",
    "                    periods=len(ts[\"target\"]),\n",
    "                ),\n",
    "            )\n",
    "            for ts in data_list\n",
    "        ]\n",
    "\n",
    "        tseries_df = pd.concat(data_list, axis=1)\n",
    "        tseries_df[\"time_index\"] = np.arange(tseries_df.shape[0])\n",
    "\n",
    "        ts_df = tseries_df.melt(\"time_index\")\n",
    "        ts_df = ts_df.rename(columns={\"variable\": \"group_id\"})\n",
    "\n",
    "        unique_times = ts_df[\"time_index\"].sort_values().unique()\n",
    "\n",
    "        tr_ind, ts_ind = train_test_split(\n",
    "            unique_times, test_size=self.test_size, shuffle=False\n",
    "        )\n",
    "\n",
    "        tr_ind, vl_ind = train_test_split(tr_ind, test_size=0.1, shuffle=False)\n",
    "\n",
    "        training_df = ts_df.loc[ts_df[\"time_index\"].isin(tr_ind), :]\n",
    "        validation_df = ts_df.loc[ts_df[\"time_index\"].isin(vl_ind), :]\n",
    "        test_df = ts_df.loc[ts_df[\"time_index\"].isin(ts_ind), :]\n",
    "\n",
    "        self.target_scaler.fit(training_df)\n",
    "\n",
    "        training_df = self.target_scaler.transform(training_df)\n",
    "        validation_df = self.target_scaler.transform(validation_df)\n",
    "        test_df = self.target_scaler.transform(test_df)\n",
    "\n",
    "        self.training = TimeSeriesDataSet(\n",
    "            data=training_df,\n",
    "            time_idx=\"time_index\",\n",
    "            target=\"value\",\n",
    "            group_ids=[\"group_id\"],\n",
    "            max_encoder_length=self.n_lags,\n",
    "            max_prediction_length=self.horizon,\n",
    "            time_varying_unknown_reals=[\"value\"],\n",
    "        )\n",
    "\n",
    "        self.validation = TimeSeriesDataSet.from_dataset(self.training, validation_df)\n",
    "        self.test = TimeSeriesDataSet.from_dataset(self.training, test_df)\n",
    "        self.predict_set = TimeSeriesDataSet.from_dataset(\n",
    "            self.training, ts_df, predict=True\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.training.to_dataloader(batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.validation.to_dataloader(batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test.to_dataloader(batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return self.predict_set.to_dataloader(batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff306f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decoder_cat': tensor([], size=(4, 7, 0), dtype=torch.int64),\n",
      " 'decoder_cont': tensor([[[-0.5943],\n",
      "         [-0.4604],\n",
      "         [-0.4151],\n",
      "         [-0.3232],\n",
      "         [-0.3250],\n",
      "         [-0.4820],\n",
      "         [-0.4668]],\n",
      "\n",
      "        [[-0.4604],\n",
      "         [-0.4151],\n",
      "         [-0.3232],\n",
      "         [-0.3250],\n",
      "         [-0.4820],\n",
      "         [-0.4668],\n",
      "         [-0.4960]],\n",
      "\n",
      "        [[-0.4151],\n",
      "         [-0.3232],\n",
      "         [-0.3250],\n",
      "         [-0.4820],\n",
      "         [-0.4668],\n",
      "         [-0.4960],\n",
      "         [-0.4147]],\n",
      "\n",
      "        [[-0.3232],\n",
      "         [-0.3250],\n",
      "         [-0.4820],\n",
      "         [-0.4668],\n",
      "         [-0.4960],\n",
      "         [-0.4147],\n",
      "         [-0.3077]]]),\n",
      " 'decoder_lengths': tensor([7, 7, 7, 7]),\n",
      " 'decoder_target': tensor([[-2.3222, -2.2711, -2.2538, -2.2187, -2.2194, -2.2793, -2.2735],\n",
      "        [-2.2711, -2.2538, -2.2187, -2.2194, -2.2793, -2.2735, -2.2847],\n",
      "        [-2.2538, -2.2187, -2.2194, -2.2793, -2.2735, -2.2847, -2.2537],\n",
      "        [-2.2187, -2.2194, -2.2793, -2.2735, -2.2847, -2.2537, -2.2128]]),\n",
      " 'decoder_time_idx': tensor([[ 7,  8,  9, 10, 11, 12, 13],\n",
      "        [ 8,  9, 10, 11, 12, 13, 14],\n",
      "        [ 9, 10, 11, 12, 13, 14, 15],\n",
      "        [10, 11, 12, 13, 14, 15, 16]]),\n",
      " 'encoder_cat': tensor([], size=(4, 7, 0), dtype=torch.int64),\n",
      " 'encoder_cont': tensor([[[-0.5589],\n",
      "         [-0.5357],\n",
      "         [-0.4519],\n",
      "         [-0.3180],\n",
      "         [-0.3861],\n",
      "         [-0.5057],\n",
      "         [-0.5258]],\n",
      "\n",
      "        [[-0.5357],\n",
      "         [-0.4519],\n",
      "         [-0.3180],\n",
      "         [-0.3861],\n",
      "         [-0.5057],\n",
      "         [-0.5258],\n",
      "         [-0.5943]],\n",
      "\n",
      "        [[-0.4519],\n",
      "         [-0.3180],\n",
      "         [-0.3861],\n",
      "         [-0.5057],\n",
      "         [-0.5258],\n",
      "         [-0.5943],\n",
      "         [-0.4604]],\n",
      "\n",
      "        [[-0.3180],\n",
      "         [-0.3861],\n",
      "         [-0.5057],\n",
      "         [-0.5258],\n",
      "         [-0.5943],\n",
      "         [-0.4604],\n",
      "         [-0.4151]]]),\n",
      " 'encoder_lengths': tensor([7, 7, 7, 7]),\n",
      " 'encoder_target': tensor([[-2.3087, -2.2998, -2.2679, -2.2168, -2.2428, -2.2884, -2.2961],\n",
      "        [-2.2998, -2.2679, -2.2168, -2.2428, -2.2884, -2.2961, -2.3222],\n",
      "        [-2.2679, -2.2168, -2.2428, -2.2884, -2.2961, -2.3222, -2.2711],\n",
      "        [-2.2168, -2.2428, -2.2884, -2.2961, -2.3222, -2.2711, -2.2538]]),\n",
      " 'groups': tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]),\n",
      " 'target_scale': tensor([[-2.0954,  0.3816],\n",
      "        [-2.0954,  0.3816],\n",
      "        [-2.0954,  0.3816],\n",
      "        [-2.0954,  0.3816]])}\n",
      "(tensor([[-2.3222, -2.2711, -2.2538, -2.2187, -2.2194, -2.2793, -2.2735],\n",
      "        [-2.2711, -2.2538, -2.2187, -2.2194, -2.2793, -2.2735, -2.2847],\n",
      "        [-2.2538, -2.2187, -2.2194, -2.2793, -2.2735, -2.2847, -2.2537],\n",
      "        [-2.2187, -2.2194, -2.2793, -2.2735, -2.2847, -2.2537, -2.2128]]),\n",
      " None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volody/code/study-py/ts-pytorch/.venv/lib/python3.13/site-packages/pytorch_forecasting/data/timeseries/_timeseries.py:1712: UserWarning: If predicting, no randomization should be possible - setting stop_randomization=True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "datamodule = GlobalDataModule(\n",
    "    data=dataset, n_lags=N_LAGS, horizon=HORIZON, test_size=0.2, batch_size=4\n",
    ")\n",
    "\n",
    "datamodule.setup()\n",
    "\n",
    "x, y = next(iter(datamodule.train_dataloader()))\n",
    "\n",
    "pprint(x)\n",
    "pprint(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
